## 往期回顾

在上一篇文章中，我们已经掌握了机器学习的基本套路，对模型、目标函数、优化算法这些概念有了一定程度的理解，而且已经会训练单个的感知器或者线性单元了。在这篇文章中，我们将把这些单独的单元按照一定的规则相互连接在一起形成**神经网络**，从而奇迹般的获得了强大的学习能力。我们还将介绍这种网络的训练算法：**反向传播算法**。最后，我们依然用代码实现一个神经网络。如果您能坚持到本文的结尾，将会看到我们用自己实现的神经网络去识别手写数字。现在请做好准备，您即将双手触及到深度学习的大门。

## 神经元

神经元和感知器本质上是一样的，只不过我们说感知器的时候，它的激活函数是**阶跃函数**；而当我们说神经元时，激活函数往往选择为sigmoid函数或tanh函数。如下图所示：

![img](http://upload-images.jianshu.io/upload_images/2256672-49f06e2e9d3eb29f.gif)

计算一个神经元的输出的方法和计算一个感知器的输出是一样的。假设神经元的输入是向量，权重向量是(偏置项是)，激活函数是sigmoid函数，则其输出：



式



sigmoid函数的定义如下：







将其带入前面的式子，得到







sigmoid函数是一个非线性函数，值域是(0,1)。函数图像如下图所示

![img](http://upload-images.jianshu.io/upload_images/2256672-e7e64f57dc6b1c64.jpg)

sigmoid函数的导数是：



令则



可以看到，sigmoid函数的导数非常有趣，它可以用sigmoid函数自身来表示。这样，一旦计算出sigmoid函数的值，计算它的导数的值就非常方便。

## 神经网络是啥

![img](http://upload-images.jianshu.io/upload_images/2256672-92111b104ce0d571.jpeg)

神经网络其实就是按照**一定规则**连接起来的多个**神经元**。上图展示了一个**全连接(full connected, FC)**神经网络，通过观察上面的图，我们可以发现它的规则包括：

- 神经元按照**层**来布局。最左边的层叫做**输入层**，负责接收输入数据；最右边的层叫**输出层**，我们可以从这层获取神经网络输出数据。输入层和输出层之间的层叫做**隐藏层**，因为它们对于外部来说是不可见的。
- 同一层的神经元之间没有连接。
- 第N层的每个神经元和第N-1层的**所有**神经元相连(这就是full connected的含义)，第N-1层神经元的输出就是第N层神经元的输入。
- 每个连接都有一个**权值**。

上面这些规则定义了全连接神经网络的结构。事实上还存在很多其它结构的神经网络，比如卷积神经网络(CNN)、循环神经网络(RNN)，他们都具有不同的连接规则。

## 计算神经网络的输出

神经网络实际上就是一个输入向量到输出向量的函数，即：







根据输入计算神经网络的输出，需要首先将输入向量的每个元素的值赋给神经网络的输入层的对应神经元，然后根据**式1**依次向前计算每一层的每个神经元的值，直到最后一层输出层的所有神经元的值计算完毕。最后，将输出层每个神经元的值串在一起就得到了输出向量。

接下来举一个例子来说明这个过程，我们先给神经网络的每个单元写上编号。

![img](http://upload-images.jianshu.io/upload_images/2256672-bfbb364740f898d1.png)

如上图，输入层有三个节点，我们将其依次编号为1、2、3；隐藏层的4个节点，编号依次为4、5、6、7；最后输出层的两个节点编号为8、9。因为我们这个神经网络是**全连接**网络，所以可以看到每个节点都和**上一层的所有节点**有连接。比如，我们可以看到隐藏层的节点4，它和输入层的三个节点1、2、3之间都有连接，其连接上的权重分别为。那么，我们怎样计算节点4的输出值呢？

为了计算节点4的输出值，我们必须先得到其所有上游节点（也就是节点1、2、3）的输出值。节点1、2、3是**输入层**的节点，所以，他们的输出值就是输入向量本身。按照上图画出的对应关系，可以看到节点1、2、3的输出值分别是。我们要求**输入向量的维度和输入层神经元个数相同**，而输入向量的某个元素对应到哪个输入节点是可以自由决定的，你偏非要把赋值给节点2也是完全没有问题的，但这样除了把自己弄晕之外，并没有什么价值。

一旦我们有了节点1、2、3的输出值，我们就可以根据**式1**计算节点4的输出值：







上式的是节点4的**偏置项**，图中没有画出来。而分别为节点1、2、3到节点4连接的权重，在给权重编号时，我们把目标节点的编号放在前面，把源节点的编号放在后面。

同样，我们可以继续计算出节点5、6、7的输出值。这样，隐藏层的4个节点的输出值就计算完成了，我们就可以接着计算输出层的节点8的输出值：







同理，我们还可以计算出的值。这样输出层所有节点的输出值计算完毕，我们就得到了在输入向量时，神经网络的输出向量。这里我们也看到，**输出向量的维度和输出层神经元个数相同**。

### 神经网络的矩阵表示

神经网络的计算如果用矩阵来表示会很方便（当然逼格也更高），我们先来看看隐藏层的矩阵表示。

首先我们把隐藏层4个节点的计算依次排列出来：







接着，定义网络的输入向量和隐藏层每个节点的权重向量。令







代入到前面的一组式子，得到：







现在，我们把上述计算的四个式子写到一个矩阵里面，每个式子作为矩阵的一行，就可以利用矩阵来表示它们的计算了。令







带入前面的一组式子，得到



式



在**式2**中，是激活函数，在本例中是函数；是某一层的权重矩阵；是某层的输入向量；是某层的输出向量。**式2**说明神经网络的每一层的作用实际上就是先将输入向量**左乘**一个数组进行线性变换，得到一个新的向量，然后再对这个向量**逐元素**应用一个激活函数。

每一层的算法都是一样的。比如，对于包含一个输入层，一个输出层和三个隐藏层的神经网络，我们假设其权重矩阵分别为，每个隐藏层的输出分别是，神经网络的输入为，神经网络的输入为，如下图所示：

![img](http://upload-images.jianshu.io/upload_images/2256672-c1388dc8fdcce427.png)

则每一层的输出向量的计算可以表示为：







这就是神经网络输出值的计算方法。

## 神经网络的训练

现在，我们需要知道一个神经网络的每个连接上的权值是如何得到的。我们可以说神经网络是一个**模型**，那么这些权值就是模型的**参数**，也就是模型要学习的东西。然而，一个神经网络的连接方式、网络的层数、每层的节点数这些参数，则不是学习出来的，而是人为事先设置的。对于这些人为设置的参数，我们称之为**超参数(Hyper-Parameters)**。

接下来，我们将要介绍神经网络的训练算法：反向传播算法。

### 反向传播算法(Back Propagation)

我们首先直观的介绍反向传播算法，最后再来介绍这个算法的推导。当然读者也可以完全跳过推导部分，因为即使不知道如何推导，也不影响你写出来一个神经网络的训练代码。事实上，现在神经网络成熟的开源实现多如牛毛，除了练手之外，你可能都没有机会需要去写一个神经网络。

我们以**监督学习**为例来解释反向传播算法。在[零基础入门深度学习(2) - 线性单元和梯度下降](https://www.zybuluo.com/hanbingtao/note/448086)一文中我们介绍了什么是**监督学习**，如果忘记了可以再看一下。另外，我们设神经元的激活函数为函数(不同激活函数的计算公式不同，详情见[反向传播算法的推导](https://www.zybuluo.com/hanbingtao/note/476663#an1)一节)。

我们假设每个训练样本为，其中向量是训练样本的特征，而是样本的目标值。

![img](http://upload-images.jianshu.io/upload_images/2256672-6f27ced45cf5c0d8.png)

首先，我们根据上一节介绍的算法，用样本的特征，计算出神经网络中每个隐藏层节点的输出，以及输出层每个节点的输出。

然后，我们按照下面的方法计算出每个节点的误差项：

- 对于输出层节点，



式



其中，是节点的误差项，是节点的**输出值**，是样本对应于节点的**目标值**。举个例子，根据上图，对于输出层节点8来说，它的输出值是，而样本的目标值是，带入上面的公式得到节点8的误差项应该是：







- 对于隐藏层节点，



式



其中，是节点的输出值，是节点到它的下一层节点的连接的权重，是节点的下一层节点的误差项。例如，对于隐藏层节点4来说，计算方法如下：







最后，更新每个连接上的权值：



式



其中，是节点到节点的权重，是一个成为**学习速率**的常数，是节点的误差项，是节点传递给节点的输入。例如，权重的更新方法如下：







类似的，权重的更新方法如下：







偏置项的输入值永远为1。例如，节点4的偏置项应该按照下面的方法计算：







我们已经介绍了神经网络每个节点误差项的计算和权重更新方法。显然，计算一个节点的误差项，需要先计算每个与其相连的下一层节点的误差项。这就要求误差项的计算顺序必须是从输出层开始，然后反向依次计算每个隐藏层的误差项，直到与输入层相连的那个隐藏层。这就是反向传播算法的名字的含义。当所有节点的误差项计算完毕后，我们就可以根据**式5**来更新所有的权重。

以上就是基本的反向传播算法，并不是很复杂，您弄清楚了么？

### 反向传播算法的推导

反向传播算法其实就是链式求导法则的应用。然而，这个如此简单且显而易见的方法，却是在Roseblatt提出感知器算法将近30年之后才被发明和普及的。对此，Bengio这样回应道：

> 很多看似显而易见的想法只有在事后才变得显而易见。

接下来，我们用链式求导法则来推导反向传播算法，也就是上一小节的**式3**、**式4**、**式5**。

***前方高能预警——接下来是数学公式重灾区，读者可以酌情阅读，不必强求。\***

按照机器学习的通用套路，我们先确定神经网络的目标函数，然后用**随机梯度下降**优化算法去求目标函数最小值时的参数值。

我们取网络所有输出层节点的误差平方和作为目标函数：







其中，表示是样本的误差。

然后，我们用文章[零基础入门深度学习(2) - 线性单元和梯度下降](https://www.zybuluo.com/hanbingtao/note/448086)中介绍的**随机梯度下降**算法对目标函数进行优化：







随机梯度下降算法也就是需要求出误差对于每个权重的偏导数（也就是梯度），怎么求呢？

![img](http://upload-images.jianshu.io/upload_images/2256672-6f27ced45cf5c0d8.png)

观察上图，我们发现权重仅能通过影响节点的输入值影响网络的其它部分，设是节点的**加权输入**，即







是的函数，而是的函数。根据链式求导法则，可以得到：






上式中，是节点传递给节点的输入值，也就是节点的输出值。



对于的推导，需要区分**输出层**和**隐藏层**两种情况。

#### 输出层权值训练

对于**输出层**来说，仅能通过节点的输出值来影响网络其它部分，也就是说是的函数，而是的函数，其中。所以我们可以再次使用链式求导法则：







考虑上式第一项:







考虑上式第二项：







将第一项和第二项带入，得到：







如果令，也就是一个节点的误差项是网络误差对这个节点输入的偏导数的相反数。带入上式，得到：







上式就是**式3**。

将上述推导带入随机梯度下降公式，得到：







上式就是**式5**。

#### 隐藏层权值训练

现在我们要推导出隐藏层的。

首先，我们需要定义节点的所有直接下游节点的集合。例如，对于节点4来说，它的直接下游节点是节点8、节点9。可以看到只能通过影响再影响。设是节点的下游节点的输入，则是的函数，而是的函数。因为有多个，我们应用全导数公式，可以做出如下推导：







因为，带入上式得到：







上式就是**式4**。

***——数学公式警报解除——\***

至此，我们已经推导出了反向传播算法。需要注意的是，我们刚刚推导出的训练规则是根据激活函数是sigmoid函数、平方和误差、全连接网络、随机梯度下降优化算法。如果激活函数不同、误差计算方式不同、网络连接结构不同、优化算法不同，则具体的训练规则也会不一样。但是无论怎样，训练规则的推导方式都是一样的，应用链式求导法则进行推导即可。

## 神经网络的实现

> 完整代码请参考GitHub: https://github.com/hanbt/learn_dl/blob/master/bp.py (python2.7)

现在，我们要根据前面的算法，实现一个基本的全连接神经网络，这并不需要太多代码。我们在这里依然采用面向对象设计。

首先，我们先做一个基本的模型：

![img](http://upload-images.jianshu.io/upload_images/2256672-2fbae2ee722fbef9.png?imageMogr2/auto-orient/strip|imageView2/2/w/360)

如上图，可以分解出5个领域对象来实现神经网络：

- *Network* 神经网络对象，提供API接口。它由若干层对象组成以及连接对象组成。
- *Layer* 层对象，由多个节点组成。
- *Node* 节点对象计算和记录节点自身的信息(比如输出值、误差项等)，以及与这个节点相关的上下游的连接。
- *Connection* 每个连接对象都要记录该连接的权重。
- *Connections* 仅仅作为Connection的集合对象，提供一些集合操作。