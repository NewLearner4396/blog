import{_ as l,c as s,e as i,a as n,b as t,d as r,r as o,o as d}from"./app-BSRiWrsC.js";const c={},p={href:"https://www.zybuluo.com/hanbingtao/note/485480",target:"_blank",rel:"noopener noreferrer"};function h(g,e){const a=o("ExternalLinkIcon");return d(),s("div",null,[e[1]||(e[1]=i(`<h2 id="卷积神经网络cnn" tabindex="-1"><a class="header-anchor" href="#卷积神经网络cnn"><span>卷积神经网络CNN</span></a></h2><h3 id="示意图" tabindex="-1"><a class="header-anchor" href="#示意图"><span>示意图</span></a></h3><p><img src="http://imagebed.krins.cloud/api/image/ZN8P8T04.png" alt="典型卷积神经网络之AlexNet"></p><h3 id="优势" tabindex="-1"><a class="header-anchor" href="#优势"><span>优势</span></a></h3><h4 id="全连接网络-vs-卷积网络" tabindex="-1"><a class="header-anchor" href="#全连接网络-vs-卷积网络"><span>全连接网络 VS 卷积网络</span></a></h4><p>全连接神经网络之所以不太适合图像识别任务，主要有以下几个方面的问题：</p><ul><li><strong>参数数量太多</strong> 考虑一个输入1000<em>1000像素的图片(一百万像素，现在已经不能算大图了)，输入层有1000</em>1000=100万节点。假设第一个隐藏层有100个节点(这个数量并不多)，那么仅这一层就有(1000*1000+1)*100=1亿参数，这实在是太多了！我们看到图像只扩大一点，参数数量就会多很多，因此它的扩展性很差。</li><li><strong>没有利用像素之间的位置信息</strong> 对于图像识别任务来说，每个像素和其周围像素的联系是比较紧密的，和离得很远的像素的联系可能就很小了。如果一个神经元和上一层所有神经元相连，那么就相当于对于一个像素来说，把图像的所有像素都等同看待，这不符合前面的假设。当我们完成每个连接权重的学习之后，最终可能会发现，有大量的权重，它们的值都是很小的(也就是这些连接其实无关紧要)。努力学习大量并不重要的权重，这样的学习必将是非常低效的。</li><li><strong>网络层数限制</strong> 我们知道网络层数越多其表达能力越强，但是通过梯度下降方法训练深度全连接神经网络很困难，因为全连接神经网络的梯度很难传递超过3层。因此，我们不可能得到一个很深的全连接神经网络，也就限制了它的能力。</li></ul><p>那么，卷积神经网络又是怎样解决这个问题的呢？主要有三个思路：</p><ul><li><strong>局部连接</strong> 这个是最容易想到的，每个神经元不再和上一层的所有神经元相连，而只和一小部分神经元相连。这样就减少了很多参数。</li><li><strong>权值共享</strong> 一组连接可以共享同一个权重，而不是每个连接有一个不同的权重，这样又减少了很多参数。</li><li><strong>下采样</strong> 可以使用池化来减少每层的样本数，进一步减少参数数量，同时还可以提升模型的鲁棒性。</li></ul><p>对于图像识别任务来说，卷积神经网络通过尽可能保留重要的参数，去掉大量不重要的参数，来达到更好的学习效果。</p><h3 id="结构" tabindex="-1"><a class="header-anchor" href="#结构"><span>结构</span></a></h3><p>一个卷积神经网络由若干<strong>卷积层</strong>、<strong>Pooling层</strong>、<strong>全连接层</strong>组成。你可以构建各种不同的卷积神经网络，它的常用架构模式为：</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text" data-title="text"><pre><code><span class="line">INPUT -&gt; [[CONV]*N -&gt; POOL?]*M -&gt; [FC]*K</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>也就是N个卷积层叠加，然后(可选)叠加一个Pooling层，重复这个结构M次，最后叠加K个全连接层。</p><h3 id="卷积" tabindex="-1"><a class="header-anchor" href="#卷积"><span>卷积</span></a></h3><h3 id="池化" tabindex="-1"><a class="header-anchor" href="#池化"><span>池化</span></a></h3><h3 id="非线性激活" tabindex="-1"><a class="header-anchor" href="#非线性激活"><span>非线性激活</span></a></h3><h4 id="一个新的激活函数——relu" tabindex="-1"><a class="header-anchor" href="#一个新的激活函数——relu"><span>一个新的激活函数——Relu</span></a></h4><p>最近几年卷积神经网络中，激活函数往往不选择sigmoid或tanh函数，而是选择relu函数。</p><p><img src="http://imagebed.krins.cloud/api/image/HZHR046T.png#pic_center" alt="Relu函数"></p><p><img src="http://imagebed.krins.cloud/api/image/PH666B00.png#pic_center" alt="img"></p><p>Relu函数作为激活函数，有下面几大优势：</p><ul><li><strong>速度快</strong> 和sigmoid函数需要计算指数和倒数相比，relu函数其实就是一个max(0,x)，计算代价小很多。</li><li><strong>减轻梯度消失问题</strong> 回忆一下计算梯度的公式。其中，是sigmoid函数的导数。在使用反向传播算法进行梯度计算时，每经过一层sigmoid神经元，梯度就要乘上一个。从下图可以看出，函数最大值是1/4。因此，乘一个会导致梯度越来越小，这对于深层网络的训练是个很大的问题。而relu函数的导数是1，不会导致梯度变小。当然，激活函数仅仅是导致梯度减小的一个因素，但无论如何在这方面relu的表现强于sigmoid。使用relu激活函数可以让你训练更深的网络。</li></ul><p><img src="http://imagebed.krins.cloud/api/image/2JD4P2B2.png#pic_center" alt="img"></p><ul><li><strong>稀疏性</strong> 通过对大脑的研究发现，大脑在工作的时候只有大约5%的神经元是激活的，而采用sigmoid激活函数的人工神经网络，其激活率大约是50%。有论文声称人工神经网络在15%-30%的激活率时是比较理想的。因为relu函数在输入小于0时是完全不激活的，因此可以获得一个更低的激活率。</li></ul><h3 id="梯度反向传播" tabindex="-1"><a class="header-anchor" href="#梯度反向传播"><span>梯度反向传播</span></a></h3><h3 id="参考资料" tabindex="-1"><a class="header-anchor" href="#参考资料"><span>参考资料</span></a></h3>`,27)),n("p",null,[n("a",p,[e[0]||(e[0]=t("零基础入门深度学习(4) - 卷积神经网络")),r(a)])])])}const m=l(c,[["render",h]]),N=JSON.parse('{"path":"/blogs/Miscellaneous/CNN.html","title":"卷积神经网络CNN的组成","lang":"en-US","frontmatter":{"title":"卷积神经网络CNN的组成","date":"2022-09-22T00:00:00.000Z","tags":["CNN","DeepLearning"],"categories":["Miscellaneous"]},"headers":[{"level":2,"title":"卷积神经网络CNN","slug":"卷积神经网络cnn","link":"#卷积神经网络cnn","children":[{"level":3,"title":"示意图","slug":"示意图","link":"#示意图","children":[]},{"level":3,"title":"优势","slug":"优势","link":"#优势","children":[]},{"level":3,"title":"结构","slug":"结构","link":"#结构","children":[]},{"level":3,"title":"卷积","slug":"卷积","link":"#卷积","children":[]},{"level":3,"title":"池化","slug":"池化","link":"#池化","children":[]},{"level":3,"title":"非线性激活","slug":"非线性激活","link":"#非线性激活","children":[]},{"level":3,"title":"梯度反向传播","slug":"梯度反向传播","link":"#梯度反向传播","children":[]},{"level":3,"title":"参考资料","slug":"参考资料","link":"#参考资料","children":[]}]}],"git":{"createdTime":1744420073000,"updatedTime":1744420073000,"contributors":[{"name":"NewLearner4396","email":"NewLearner4396@users.noreply.github.com","commits":1}]},"filePathRelative":"blogs/Miscellaneous/CNN.md"}');export{m as comp,N as data};
